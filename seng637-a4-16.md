**SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group: 16           |
|-------------------------------|
| Sayan                     |   
| Md Afif Al               |   
| Sutirtha                   |   
| Abhijit              |
| Moshfiq-Us-Saleheen  |

# Introduction

In this laboratory exercise, we split our focus into two main parts: mutation testing and GUI testing.

First up, we delved into mutation testing using the JFreeChart framework. Specifically, we concentrated on the Range and DataUtilities classes. Using tools like Eclipse, JUnit, and Pitest, we brought over the tests we developed earlier in assignment #3. With these tools, we introduced changes, known as mutants, into the code to see how well our tests could identify these alterations. Our main target was to enhance our tests to detect more of these mutants, ideally increasing our mutation score by at least 10%. However, we understood that achieving this might be challenging due to the nature of the mutants created by Pitest.

The second part of our exercise involved GUI testing with the Selenium tool. For this, we chose the Home Depot website as our testing ground. We approached this as if we were typical users looking to purchase or rent items from the site, which included tasks like creating a user account. We planned and executed a total of ten test scenarios using Selenium to interact with and test the website’s graphical user interface.

Overall, this lab was designed to provide hands-on experience with two critical areas of software testing: evaluating the thoroughness of unit tests through mutation testing and assessing user interface quality through GUI testing.

# Analysis of 10 Mutants of the Range class 

We picked 10 different types of mutants of the Range class for this analysis. We focused on covering as many different types as we can from different functions from the class.

### Mutation 1 (Line 110, Mutation 7)
#### Mutation Applied: Decremented (--a) double fieldlower → KILLED

Target Method: `public double getLowerBound()`
```
return this.lower
```
The mutation is applied in the above line and it suggests that the lower value is decremented by 1 before returning. So, for a Range(-1, 2), the method would return `-1 - 1 = -2`. However, our test suite perfectly handles the issue as it is obvious that we would assert equal to -1 for getLowerBound() call. Hence, this mutation was killed. 
```
return this.lower - 1
```
For example, a Range with (-5, 5) and a value -5, the method would return false even though it is in the same range. However, our test suite is successfull to assert such issues hence the mutation was killed.

### Mutation 2 (Line 147, Mutation 40)
#### Mutation Applied: Incremented (a++) double field lower → SURVIVED

Target Method: `public double getCentralValue()`
```java
return this.lower / 2.0 + this.upper / 2.0;
```
The mutation is applied in the above line and it suggests that the lower value is post-incremented by 1. The resultant line would look like this:
```java
return this.lower++ / 2.0 + this.upper / 2.0;
```
However, as it is a post-increment and the increment operation only happens after the whole line is executed there is no change in the result and the mutation survives. We can evaluate this issue with a simple print statement:
```java
int a = 2;
System.out.print(a); // 2
System.out.print(a++); // Still a 2
```

### Mutation 3 (Line 159, Mutation 1)
#### Mutation Applied: changed conditional boundary → KILLED

Target Method: `public boolean contains(double value)`
```java
if (value < this.lower) {...}
```
The mutation suggests that a change has been made in the comparison operation from "less than" to "less than or equal" in the code snippet provided within the contains method. So the above line became:
```java
if (value <= this.lower) {...}
```
For example, a Range with (-5, 5) and a value -5, the method would return false even though it is in the same range. However, our test suite is successfull to assert such issues hence the mutation was killed.


### Mutation 4 (Line 198, Mutation 1)
#### Mutation Applied: replaced boolean return with false for org/jfree/data/Range::intersects → KILLED

Target Method: `public boolean intersects(Range range)`
```java
return intersects(range.getLowerBound(), range.getUpperBound());
```
This mutation is essentially replacing the boolean method call with a `false`.
```java
return false
```
For example, event though a base Range with (-5, 5) would intersect with a Range(5, 10) the method would still return `false`. However, it is straightforward that our test case expects a `true` in such cases and this mutation was killed.

### Mutation 5 (Line 239, Mutation 1)
#### Mutation Applied: negated conditional → KILLED

Target Method: `public static Range combine(Range range1, Range range2)`
```java
if (range1 == null)
  return range2;
```
This mutation is negating the conditional value. In simpler words, the condition will now be changed to the cases when the object is not null.
```java
if (range1 != null)
  return range2;
```
With this mutated condition, even when the range1 is not null, the method would blindly return range2. Howeever, in our test cases we expect range2 to be returned only when range1 is null which would fail the test case and kill this mutant.

### Mutation 6 (Line 240, Mutation 1)
#### Mutation Applied: replaced return value with null for org/jfree/data/Range::combine → KILLED

Target Method: `public static Range combine(Range range1, Range range2)`
```java
return range2;
```
This mutation would return a null on combine method call regardless the scenario essentially turning the return statement like below.
```java
return null;
```
With this mutated condition, the method would return null on every call regardless of the input parameters. But our test methods like `testCombineWithFirstRangeNull` would expect the method to return the second range when the first range is null. This would fail the test case as well as killing the mutation.

### Mutation 7 (Line 351, Mutation 1)
#### Mutation Applied: removed call to org/jfree/chart/util/ParamChecks::nullNotPermitted → SURVIVED

Target Method: `public static Range expand(Range range, double lowerMargin, double upperMargin)`

```java
ParamChecks.nullNotPermitted(range, "range");
```
This mutation removes the call to the above line or simply ignores it. The method call throws IllegalArgumentException if the parameter range is null. Since there was no test case that tests base as null for this method, the removal of the call to ParamChecks.nullNotPermitted() went unnoticed by the tests and the mutation was survived.

### Mutation 8 (Line 409, Mutation 2)
#### Mutation Applied: Substituted 0.0 with 1.0 → KILLED

Target Method: `private static double shiftWithNoZeroCrossing(double value, double delta)`
```java
if (value > 0.0) {
```
This mutation would change the above condition the following one:
```java
if (value > 1.0) {
```
With this mutation, the condition would be true only if the value is greater than 1.0. However, our test cases include such scenario where changing it to 1.0 fails the test case. Hence, it kills the mutation.

### Mutation 9 (Line 448, Mutation 3)
#### Mutation Applied: removed conditional - replaced equality check with true → KILLED

Target Method: `public boolean equals(Object obj)`
```java
if (!(obj instanceof Range)) {
    return false;
}
```
In the base condition, we are checking if the passed down object is an instance of Range. If not, we are returning false. However, the mutation changes the condition to the following:
```java
if (true) {
  return false;
}
```
The condition with this condition is always true and returns false. Even in case when both of the ranges are same (for example, the test case `testEqualsWithIdenticalRanges`) the method would return false which clearly fails the test case killing this mutation.

### Mutation 10 (Line 485, Mutation )
#### Mutation Applied: Substituted 29 with 1 → SURVIVED

Target Method: `public int hashCode()`
```java
result = 29 * result + (int) (temp ^ (temp >>> 32));
```
In the base condition, we are performing arithmetic operations with specific constants like 29 and 32. Now, if we change the constants, the result would also change. For example, if we replace 29 with 1, the equation becomes:
```java
result = 1 * result + (int) (temp ^ (temp >>> 32));
```
The condition now would return a different result. However, our current test cases somehow overlooked this scenario and the mutation survives.



# Report all the statistics and the mutation score for each test class

## Mutation Statistics - Before
* Range Class

![](./media/RangeClass-Mutation-1.png)
![](./media/RangeClass-Mutation-2.png)

* DataUtilities Class

![](./media/DataUtilitiesClass-Mutation-1.png)
![](./media/DataUtilitiesClass-Mutation-2.png)

## Mutation Statistics - After
* Range Class

![](./media/RangeClass-Mutation-1-After.png)
![](./media/RangeClass-Mutation-2-After.png)

* DataUtilities Class

![](./media/DataUtilities-Mutation-1-After.png)
![](./media/DataUtilities-Mutation-2-After.png)

Here, are the complete statistics of the mutation test for both the class `Range` & `DataUtilities`.

# Analysis drawn on the effectiveness of each of the test classes

Here is the analysis of the test classes for both `Range` and `DataUtilities` in Assignment 4:

### DataUtilities.java Analysis:
- **Line Coverage:** 89% (85 out of 96 lines)
- **Mutation Coverage:** 78% (686 out of 881 mutations)
- **Test Strength:** 87% (686 out of 793 effective mutations)

**Analysis:** 
The `DataUtilities` class has a high line coverage, indicating that most of the code is being exercised by the tests. However, there's still room for improvement, especially considering the mutation coverage. While 78% mutation coverage is decent, it implies that approximately 22% of the mutations were not caught by the tests, suggesting potential weaknesses or untested scenarios. The test strength is quite high, which means that when tests identify an issue (a mutation), they are generally correct. Improvements could be made by creating additional tests to cover the missed mutations, particularly focusing on edge cases or less common scenarios that might not currently be covered.

### Range.java Analysis:
- **Line Coverage:** 92% (110 out of 119 lines)
- **Mutation Coverage:** 73% (1012 out of 1381 mutations)
- **Test Strength:** 75% (1012 out of 1341 effective mutations)

**Analysis:** 
The `Range` class demonstrates excellent line coverage, showing that the tests are well-designed in terms of reaching most parts of the code. However, the mutation coverage is lower than the line coverage, suggesting that while the tests are running through most lines of code, they are not effectively detecting all potential faults. The lower test strength compared to line coverage indicates that there are areas of the logic that are not thoroughly tested. There's a significant number of undetected mutations, meaning the existing tests might be passing even when bugs are present. It's advisable to analyze which specific mutations are surviving and enhance the tests to target these areas, ensuring a more robust verification of the class's behavior.

# A discussion on the effect of equivalent mutants on mutation score accuracy

**Effect on Mutation Score Accuracy:**
1. **False Assurance:** If your mutation testing process generates a lot of equivalent mutants, it might give you a falsely high mutation score. This means you think your tests are more effective than they actually are because they're "catching" mutants that aren't real bugs.

2. **Reducing Clarity:** Equivalent mutants can make it harder to understand where your actual testing gaps are. Instead of focusing on parts of the code that are truly under-tested, you might waste time trying to "kill" mutants that are, in fact, unkillable.

3. **`Range` and `DataUtilities` Analysis:** 
   - In the `Range` class, a mutation score of 73% might be inflated by equivalent mutants. If some of these mutants don’t change the program's external behavior, your real mutation score might be lower, suggesting your tests are not as robust as they appear.
   - Conversely, in the `DataUtilities` class, a higher mutation coverage of 78% and a test strength of 87% suggest that tests are effective. However, if many equivalent mutants exist, this effectiveness could be overestimated. Reducing the number of equivalent mutants (by refining tests or adjusting the mutation testing process) could provide a more accurate measure of test quality.

# A discussion of what could have been done to improve the mutation score of the test suites

### Improving Mutation Score for `Range` Class:
1. **Cover More Lines:** The line coverage is 92%, which is pretty good. However, improving this could lead to catching more mutations. Look into the lines that aren't being tested and see if you can add tests specifically for those areas.

2. **Focus on Missed Mutations:** With a mutation coverage of 73%, there's room for improvement. Analyze which mutations weren't caught by the tests. Are they concentrated in specific methods or types of operations? Create targeted tests to address these gaps.

3. **Review Existing Tests:** Sometimes, tests are not as effective as they could be. Review your current tests to ensure they're not just passing due to coincidences. Ensure each test has a clear purpose and is testing a specific behavior.

4. **Reduce Equivalent Mutants:** If there are equivalent mutants being counted, identify them and consider excluding them from the score if they cannot be killed due to the nature of the code. This gives a more accurate reflection of the test suite's effectiveness.

### Improving Mutation Score for `DataUtilities` Class:
1. **Increase Line Coverage:** With an 89% line coverage, identify which parts of the code are not being tested. Creating tests for these parts can help uncover more mutants.

2. **Analyze Mutation Coverage:** With a mutation coverage of 78%, look at the types of mutations that are surviving. Are specific operations like error handling or boundary conditions being missed? Tailor tests to these areas.

3. **Enhance Test Cases:** Strengthen your test cases so they are more precise and robust. For example, if you're testing numeric calculations, make sure to include edge cases and boundary values.

4. **Refine Test Data:** The quality of your test data can significantly impact mutation testing results. Make sure your test data covers a wide range of scenarios, especially edge cases that are likely to reveal more mutations.

5. **Continuous Review and Refinement:** Mutation testing should be an ongoing process. Regularly review and refine your tests based on new code changes, bug discoveries, and previous mutation testing results.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

Mutation testing is a way to check how good your tests are. Imagine you have a set of tests for your program. Mutation testing involves making small changes (mutations) to your program's code on purpose to create slightly wrong versions of it. Then, it runs your tests against these wrong versions. The idea is if your tests are good, they should fail because they've caught the mistakes. If the tests don't fail, it might mean they are not checking thoroughly enough.

### Advantages of Mutation Testing:

1. **Improves Test Quality**: It helps make sure your tests really are checking your code properly. By finding the parts of your code that aren't being tested well, you can write better tests.

2. **Finds Hidden Bugs**: Sometimes, it can catch mistakes in the code that were missed before because it forces the tests to look at the code in new ways.

3. **Better Code Understanding**: It can help developers understand their code better because they have to think about how changes affect the program.

### Disadvantages of Mutation Testing:

1. **Takes a Lot of Time**: Mutation testing can be very slow because it has to create a lot of wrong versions of your program and then test each one. This can take a lot longer than regular testing.

2. **Can Be Complicated**: Sometimes, the changes made during mutation testing are so small that they don't really affect anything, which can make interpreting the results tricky.

3. **Resource Intensive**: It uses a lot of computer power and memory, especially for large programs, which can be a problem if you don't have a lot of resources.

In simple terms, mutation testing can be a great way to make sure your tests are doing their job well, but it can take a lot of time and resources to do it.

# Selenium test case design process
Prior to initiating testing procedures, we conducted a thorough examination of the provided websites. We explored different functionalities of these websites. With a careful observation and exploration, we choose [Shop Smart Canada](https://shopsmartcanada.com/) as our System Under Test (SUT). Our rationale for selecting this particular website is as follows:

1. By the look of the website, we could tell it is an old website having potential existence of bugs making it a perfect test subject.
2. The website is built using PHP and follows a multi-page structure which makes it easier to imitate user interactions.
3. Additionally, we observed that other websites like [Home Depot](https://www.homedepot.ca/) dynamically generate DOM element IDs, rendering it more challenging to intercept the desired targets using Selenium.

With the SUT chosen, we compiled a list of functionalities to be tested or automated using Selenium IDE. Our emphasis was on the most frequently used features of the website. The outlined features are as follows:

- Login
- Searching with different queries
- Add or remove addresses
- Sorting products with different criteria
- Updating account information
- Adding or removing items from cart
- Creating and deleting wish list
- Adding or removing items from wish list
- Newsletter subscription

After defining the scenarios or functionalities, we created several test cases for each of the functionalities to be tested and analyze if they work as expected. For example, we expect a user can login with valid credentials but can not with an invalid one.

As the website requires authentication for different actions to be performed, we also created a test account with the following credentials and used it as required:
```
Email: vzz3wv0uf4@lettershield.com
Password: testPassword123
```
# Explain the use of assertions and checkpoints
In Selenium, assertions and checkpoints are essential for validating the behavior and the state of web elements or pages during test execution. We assert in different checkpoints to make sure everything is working as expected.

**Assertions:**
Assertions are statements that verify whether a certain condition is true or false. For example, we might assert that a certain element is present, has a specific text, or is enabled/disabled. If the assertion fails during test execution, it indicates a discrepancy between the expected and actual behavior. Quoting the Selenium documentation, there are 3 types of assertions.

>  **Assert:** If it fails, the test is stopped immediately.<br> **Verify:** If it fails, Selenium IDE logs this failure and continues with the test execution.<br>**WaitFor:** waitFor commands will first wait for a certain condition to become true.

In our testing, we used all of the assertion types.

**Checkpoints:**
Checkpoints are similar to assertions but often they are used to ensure that specific criteria are met at a particular point during test execution. For instance, you might use a checkpoint to verify the contents of a table, the presence of multiple elements on a page, or the state of a dynamic user interface component. If any of the checkpoint conditions are not met, the test execution may fail, indicating a deviation from the expected behavior.

A few examples are provided below from our generated test cases:

|Test functionality|Example of checkpoints|
|---------|----------------------|
|Login_Valid_Credentials|Asserts to the change in the header upon login or the alert message on incorrect credentials.|
|Login_Invalid_Credentials|Asserts to the alert message on incorrect credentials.|
|Account_Settings_Update_Phone_Number_Valid|Asserts text to verify if the setting is actually updated.|
|Account_Settings_Update_Phone_Number_Invalid|Asserts text to verify if the setting is actually updated.|
|Address_Valid_Data|Asserts to the new address title.|
|Address_Invalid_Data|Verifies the behaviour of the SUT whether it updates with invalid data or not.|
|Search_With_Valid_Query|Verifies the content on the page on valid search query|
|Search_With_Valid_Query|Asserts to "0 results for {query}" on invalid query.|
|Cart_Add_Item|Asserts to "Your Cart" when adding an item.|
|Cart_Remove_Item|Verifies to the text label "Your cart is empty" when everything is deleted from cart.|
|Newsletter_Subscribe_Valid|Upon subscription, asserts the confirmation text.|
|Newsletter_Subscribe_Invalid|Upon subscription attempt, asserts the error message.|
|Wish_List_Create_New_List|Asserts to list name when creation.|
|Wish_List_Delete_All_List|Asserts to text "You have no Wish Lists, add one now." when deleting all lists.| 
|Wish_List_Add_Item|Asserts to list name and waits for the added item to show up in the list.|
|Wish_List_Remove_Item|On item removal, also verifies the text - "Your Wish List is empty. When you add items to your Wish List they will appear here."|

Almost all functionalities under test also uses logout on which the test asserts or verifies to the text "You've been logged out of your account successfully."
# Testing functionality with different data
In total, we considered 9 features with 20 different inputs or test data. Whenever possible, or the test case allowed, we tested each test cases with different data. Our test cases also managed to find some existing bugs in the system. A brief table is provided with the different types of test data used.

|Test case|Test data/command|Expected|Actual|Decision|
|---------|---------|--------|------|--------|
|Login|Tested with valid credentials|Should log in|Logged in|Pass|
||Tested with invalid credentials|Should not log in and an error alert should be shown|Did not log in and an alert was shown|Pass|
|Account Settings - Update Phone Number|Valid phone number|Should update the number|Updated the number|Pass|
||Invalid phone number|Should not update the number|Updated the number|**Bug detected**|
|Search|Valid query item|Should show a list of items matching the query|Shows a list of query items|Pass|
||Invalid query item|Should not show any item but a zero response|Does not show any item but "(0) results"|Pass|
|Address - Add New Address|A valid Alberta address is provided|Should add the address|Added the address|Pass|
||An invalid address is provided|Should not add the address rather show an error message|Added the address|**Bug detected**|
|Cart|Add item to the cart|Should add the item to the cart|Added the item to the cart|Pass|
||Remove item from the cart|Should remove the item to the cart|Removed the item to the cart|Pass|
|Wish List - Creation/Deletion|Provide a wish list name to create|Should create a new wish list by the provided name|Creates the wish list|Pass|
||Remove an existing wish list|Should remove the wish list by the provided name|Removes the wish list|Pass|
|Wish List - Add/Remove Item|Add an item to a wish list|Should add the item to the wish list|Adds the item to the list|Pass|
||Remove an existing item from wish list|Should remove the item from the list|Removes the item from the list|Pass|
|Sort|Use alphabetical sorting|Should sort the items alphabetically|Sorts the item alphabetically|Pass|
||Use price-based sorting|Should sort the items by their price|Sorts the item by price|Pass|
||Use best-selling sorting|Should sort the items based on best selling|Sorts the item as expected|Pass|
||Use review-based sorting|Should sort the items based on reviews|No change is reflected in UI|**Bug detected**|
|Newsletter - Subscription|Use a valid email to subscribe|A confirmation message should be shown|Shows a confirmation message|Pass|
||Use an invalid email to subscribe|An error message should be shown|Shows an error message|Pass|

Although not expected but we were able to find a few bugs in the system. Additionally, we also believe the website is lacking some crucial features. For example, the website only provides an interface to subscribe to their newsletter but doesn't provide an option to unsubscribe.

# How the team work/effort was divided and managed

Let's delve into how the team's work and effort were managed and divided, focusing particularly on the mutation testing and Selenium testing within the context of the Range and DataUtilities classes.

### Division of Work and Management:

**1. Initial Planning and Division:**
The team initially divided the tasks into two main categories: mutation testing and Selenium testing. This approach ensured that all team members would gain experience with different types of testing techniques, which are crucial in software testing and quality assurance.

**Mutation Testing:**
- The team was split into two groups. One group focused on the `Range` class, while the other group worked on the `DataUtilities` class. This division allowed members to specialize and delve deeper into specific areas, which likely improved the quality of the testing and analysis.
- The mutation testing involved creating mutants (variations) of the code and then running tests to see if these mutants could be "killed" (detected by the tests). This process helps in identifying weaknesses in the test suite and areas of the code that might be prone to bugs.

**Selenium Testing:**
- Selenium testing was assigned to be done independently by each team member. This approach likely helped each member build their own skills and understanding of web application testing, a valuable skill in software development.
- After individual work, the results from Selenium testing were compiled and discussed collectively. This collaborative review allowed for knowledge sharing and ensured a comprehensive understanding and coverage in the final report.

**2. Collaboration and Knowledge Sharing:**
- Regular meetings were likely held to discuss progress, challenges, and strategies. These meetings would have been crucial for staying on track and ensuring everyone was aligned with the team's goals.
- Sharing findings and learning from each other was a key part of the process. By discussing different approaches and results, team members could learn from each other’s experiences, enhancing the collective knowledge.

**3. Compilation and Reporting:**
- The final step involved pulling together the findings from both mutation and Selenium testing into a comprehensive report. This required effective communication and collaboration as each member contributed their insights and results.
- The final report would not only showcase the findings from the testing efforts but also reflect the collaborative work of the team. It would include detailed analyses, challenges encountered, solutions developed, and recommendations for improving the tested software.

# Difficulties encountered, challenges overcome, and lessons learned

### Difficulties and Challenges:

**1. Mutation Testing Setup:** Encountered initial challenges in configuring the mutation testing environment correctly.

**2. Understanding PIT Reports:** Struggled to interpret mutations and navigate through the detailed PIT reports effectively.

**3. Analyzing Equivalent Mutants:** Faced difficulties in identifying and analyzing equivalent mutants due to their complex nature.

4. **Discepencies in JDK version:** The mutation test was not running due to mismatch in JDK version. Spent few hours to debug this problem. 

### Lessons Learned:

In this assignment 4, the team learned the crucial importance of proper setup and preparation. A well-configured environment lays the foundation for any successful testing process. The experience also underscored the value of patience and dedication to understanding complex testing reports and tools, like PIT. Finally, the challenge of dealing with equivalent mutants taught the team the significance of prioritization and strategic analysis in software testing. Through these experiences, the team gained a deeper appreciation for the intricacies of mutation testing and the meticulous attention to detail required for effective software quality assurance.

# Comments/feedback on the assignment itself

The assignment was a hands-on learning experience that allowed us to dive deep into mutation testing and Selenium testing. The practical approach helped us understand the complexities of software testing better.

One positive aspect was the division of tasks between mutation and Selenium testing, which ensured that all team members got a well-rounded experience. However, more guidance on interpreting PIT reports and identifying equivalent mutants would have been beneficial.

The assignment was challenging but rewarding. It pushed us to apply theoretical knowledge in real-world scenarios. However, clearer instructions and examples, especially for setting up testing environments, would have made the learning curve less steep.

Overall, the assignment was a valuable learning tool, but could be improved with more support materials and clearer guidelines.
